{
    "mcp_claude_desktop": {
        "transport": {
            "type": "stdio",
            "command": "claude-desktop-mcp-server",
            "args": [],
            "env": {}
        },
        "client_info": {
            "name": "graph-of-thoughts",
            "version": "0.0.3"
        },
        "capabilities": {
            "sampling": {}
        },
        "default_sampling_params": {
            "modelPreferences": {
                "hints": [
                    {"name": "claude-3-5-sonnet"},
                    {"name": "claude-3-haiku"}
                ],
                "costPriority": 0.3,
                "speedPriority": 0.4,
                "intelligencePriority": 0.8
            },
            "temperature": 1.0,
            "maxTokens": 4096,
            "stopSequences": [],
            "includeContext": "thisServer"
        },
        "connection_config": {
            "timeout": 30.0,
            "retry_attempts": 3,
            "retry_delay": 1.0
        },
        "cost_tracking": {
            "prompt_token_cost": 0.003,
            "response_token_cost": 0.015
        }
    },
    "mcp_vscode": {
        "transport": {
            "type": "stdio",
            "command": "code",
            "args": ["--mcp-server"],
            "env": {}
        },
        "client_info": {
            "name": "graph-of-thoughts",
            "version": "0.0.3"
        },
        "capabilities": {
            "sampling": {}
        },
        "default_sampling_params": {
            "modelPreferences": {
                "hints": [
                    {"name": "gpt-4"},
                    {"name": "gpt-3.5-turbo"}
                ],
                "costPriority": 0.5,
                "speedPriority": 0.6,
                "intelligencePriority": 0.7
            },
            "temperature": 1.0,
            "maxTokens": 4096,
            "stopSequences": [],
            "includeContext": "thisServer"
        },
        "connection_config": {
            "timeout": 30.0,
            "retry_attempts": 3,
            "retry_delay": 1.0
        },
        "cost_tracking": {
            "prompt_token_cost": 0.03,
            "response_token_cost": 0.06
        }
    },
    "mcp_cursor": {
        "transport": {
            "type": "stdio",
            "command": "cursor",
            "args": ["--mcp-server"],
            "env": {}
        },
        "client_info": {
            "name": "graph-of-thoughts",
            "version": "0.0.3"
        },
        "capabilities": {
            "sampling": {}
        },
        "default_sampling_params": {
            "modelPreferences": {
                "hints": [
                    {"name": "claude-3-5-sonnet"},
                    {"name": "gpt-4"}
                ],
                "costPriority": 0.4,
                "speedPriority": 0.5,
                "intelligencePriority": 0.8
            },
            "temperature": 1.0,
            "maxTokens": 4096,
            "stopSequences": [],
            "includeContext": "thisServer"
        },
        "connection_config": {
            "timeout": 30.0,
            "retry_attempts": 3,
            "retry_delay": 1.0
        },
        "cost_tracking": {
            "prompt_token_cost": 0.003,
            "response_token_cost": 0.015
        }
    },
    "mcp_http_server": {
        "transport": {
            "type": "http",
            "url": "http://localhost:8000/mcp",
            "headers": {
                "Content-Type": "application/json",
                "Accept": "application/json, text/event-stream"
            },
            "session_management": true
        },
        "client_info": {
            "name": "graph-of-thoughts",
            "version": "0.0.3"
        },
        "capabilities": {
            "sampling": {}
        },
        "default_sampling_params": {
            "modelPreferences": {
                "hints": [
                    {"name": "claude-3-5-sonnet"}
                ],
                "costPriority": 0.3,
                "speedPriority": 0.5,
                "intelligencePriority": 0.9
            },
            "temperature": 1.0,
            "maxTokens": 4096,
            "stopSequences": [],
            "includeContext": "allServers"
        },
        "connection_config": {
            "timeout": 60.0,
            "retry_attempts": 5,
            "retry_delay": 2.0
        },
        "cost_tracking": {
            "prompt_token_cost": 0.003,
            "response_token_cost": 0.015
        }
    }
}
